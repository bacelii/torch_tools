{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot Product Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale = math.sqrt(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # Apply mask (e.g., causal mask for autoregressive tasks)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Linear layers for query, key, value\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.attention = ScaledDotProductAttention(d_model, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "        # Linear projections\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attention_output, _ = self.attention(query, key, value, mask)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        attention_output = attention_output.view(batch_size, seq_len, d_model)\n",
    "\n",
    "        # Final linear layer\n",
    "        output = self.fc_out(attention_output)\n",
    "        return self.dropout(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, ff_dim, dropout=0.1):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForwardNetwork(d_model, ff_dim, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention + Add & Norm\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.norm1(x + attn_output)\n",
    "\n",
    "        # Feed-forward network + Add & Norm\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_seq_len, d_model)\n",
    "        pos = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(pos * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.encoding[:, :seq_len, :].to(x.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformerFromScratch(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, ff_dim, num_layers, max_seq_len, dropout=0.1):\n",
    "        super(MiniTransformerFromScratch, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Pass through transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        # Output layer\n",
    "        return self.fc_out(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniTransformerFromScratch(\n",
      "  (embedding): Embedding(5000, 128)\n",
      "  (positional_encoding): PositionalEncoding()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (attention): ScaledDotProductAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForwardNetwork(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=128, out_features=5000, bias=True)\n",
      ")\n",
      "torch.Size([8, 128, 5000])\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "vocab_size = 5000\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "num_layers = 2\n",
    "max_seq_len = 128\n",
    "\n",
    "# Instantiate model\n",
    "model = MiniTransformerFromScratch(vocab_size, d_model, num_heads, ff_dim, num_layers, max_seq_len)\n",
    "print(model)\n",
    "\n",
    "# Example input\n",
    "dummy_input = torch.randint(0, vocab_size, (8, 128))  # Batch size: 8, Seq len: 128\n",
    "output = model(dummy_input)\n",
    "print(output.shape)  # Output: [8, 128, vocab_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8416,  0.3240, -0.3630,  ...,  0.0953, -0.4463,  0.1273],\n",
       "         [ 0.4044,  0.4454,  0.2501,  ...,  0.9064, -0.6169, -0.2953],\n",
       "         [-0.3364,  0.4424, -0.5158,  ...,  0.6443, -0.4452,  0.5002],\n",
       "         ...,\n",
       "         [ 1.4170, -1.0453,  0.5021,  ...,  0.4204, -0.5039, -0.7998],\n",
       "         [ 0.5345, -0.8295, -0.9758,  ...,  0.2017, -0.6027, -0.5465],\n",
       "         [-0.3442, -0.4099, -0.0974,  ...,  0.1960, -0.6370,  0.4604]],\n",
       "\n",
       "        [[-0.3406,  0.2580, -0.4935,  ...,  0.9173, -0.2970,  0.5329],\n",
       "         [ 0.7357, -0.7487, -0.4306,  ...,  0.9837,  0.3500,  0.1061],\n",
       "         [-0.1801,  1.3837, -1.1732,  ...,  0.3499, -0.1861,  1.2262],\n",
       "         ...,\n",
       "         [ 0.0654,  0.4164, -0.3795,  ..., -0.5725, -0.6172,  1.0195],\n",
       "         [ 0.2763, -1.2397,  0.6114,  ..., -0.1280,  0.1388,  0.7767],\n",
       "         [ 0.4981,  0.4101,  0.8199,  ...,  1.1147, -0.9355,  1.1054]],\n",
       "\n",
       "        [[ 0.5130, -0.0166, -0.1584,  ..., -0.0459, -0.4756,  0.8818],\n",
       "         [-0.4252,  1.6053, -1.1361,  ...,  0.8549, -0.1645, -0.1352],\n",
       "         [ 0.5477,  1.0062, -0.6913,  ..., -0.1722, -0.6203,  0.4207],\n",
       "         ...,\n",
       "         [ 0.2549, -0.9315,  0.0926,  ...,  0.5194, -0.5849,  0.3388],\n",
       "         [-0.1117,  0.3602, -0.0764,  ...,  0.5012, -0.0031, -0.1105],\n",
       "         [ 1.4852,  0.4730,  0.7485,  ...,  1.1134, -0.1586, -0.0432]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2743, -0.2092,  0.1418,  ...,  0.3603, -0.9920,  0.3837],\n",
       "         [-0.2494,  0.0675, -0.4855,  ...,  0.6287, -0.2397,  0.6658],\n",
       "         [ 0.8563, -0.4287, -0.2142,  ...,  0.7422, -0.5069,  0.6058],\n",
       "         ...,\n",
       "         [ 0.5583,  0.2022,  0.3179,  ...,  0.1544, -0.5480,  0.2100],\n",
       "         [-0.0578, -0.6384,  0.6019,  ...,  0.2636, -0.2793, -0.3086],\n",
       "         [ 0.0795, -0.0189,  0.2549,  ..., -0.2885, -1.3258,  0.5639]],\n",
       "\n",
       "        [[-0.1315,  1.1152, -0.3916,  ...,  0.5579,  0.4398,  0.3905],\n",
       "         [-0.4408,  0.0713, -0.1252,  ...,  0.8588, -0.9113,  0.4403],\n",
       "         [ 0.9287,  0.3741,  0.6558,  ...,  0.3158,  0.3463,  0.2242],\n",
       "         ...,\n",
       "         [ 0.4067, -0.6249,  0.3175,  ...,  0.4711, -0.9114,  0.4105],\n",
       "         [-0.4583, -0.1105,  0.1637,  ..., -0.1545, -0.5768,  0.5020],\n",
       "         [ 0.8889,  0.0800,  0.3617,  ...,  0.5322, -1.6262,  0.1147]],\n",
       "\n",
       "        [[ 0.8129,  0.4227, -0.4640,  ...,  0.5946,  0.2442,  0.5225],\n",
       "         [ 0.1257,  1.1103, -0.3930,  ...,  0.1683,  0.1245,  0.4287],\n",
       "         [ 0.3581, -0.2778, -0.1466,  ...,  0.2221,  0.6059, -0.2279],\n",
       "         ...,\n",
       "         [ 0.2012, -0.6651, -0.3277,  ...,  0.7508, -0.2299,  0.1656],\n",
       "         [-0.9752, -0.2438,  0.8251,  ..., -0.1531, -0.5271,  0.0852],\n",
       "         [-0.9332,  0.1418,  0.2645,  ...,  0.3457, -0.1485,  0.6529]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
